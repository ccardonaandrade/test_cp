{
  "hash": "ba62031883192ccd0e5ab6ec86cbb0ea",
  "result": {
    "markdown": "---\ntitle: \"Notes on SQL in R\"\nsubtitle: \"This is very preliminar!!\"\n\nlang: en\n\nformat: \n  closeread-html:\n   css: msaz.css\n   code-tools: false\n   fig-format: svg\n   toc: true\n   toc_depth: 4\n   toc_float: true\n   toc-location: left\n   linkcolor: tomato\n---\n\n\nThis file is based on [Grant R. McDemott](https://grantmcdermott.com/){target=\"_blank\"} notes on [Databases](https://raw.githack.com/uo-ec607/lectures/master/16-databases/16-databases.html#Databases_101){target=\"_blank\"}. Basically, I want to be organized by following his notes step by step, and add anything (if possible!) that I might find useful in the process.\n\n## Requirements\n\n### 1. Having an account in Google Cloud Platform\n\n### 2. Loading some R packages\n\nGrant (I am going to be informal here by using his name) introduced the package [pacman](https://trinker.github.io/pacman/vignettes/Introduction_to_pacman.html){target=\"_blank\"} to me, which I find pretty cool because it reduces code.^[Please check all the functionalities of this package later on!]\n\nThe packages related to Databases management are:\n\n- *DBI*\n- *duckdb*\n- *bigrquery*\n- *glue*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Load/install packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, DBI, duckdb, bigrquery, hrbrthemes, nycflights13, glue)\n## My preferred ggplot2 theme (optional)\ntheme_set(hrbrthemes::theme_ipsum())\n```\n:::\n\n\n\n## Intro to Databases\n\n> Many “big data” problems could be more accurately described as “small data problems in disguise”.\n\nThat is going to my motto during these notes! Basically, a **database** is a structured system for storing and managing large volumes of data that cannot be held entirely in memory, allowing for efficient access, retrieval, and manipulation. They can also exist locally or remotely.\n\nWe submit a **query** to the database to tell it how to manipulate or subset the data into a more manageable form, which we can then pull into our analysis environment (R, Python, etc.).\n\nAdditionally, the data is organized in **tables** which are basically like dataframes consisting of rows and columns. A database is a list of data frames of R.\n\n> **Tip**: A table in a database is like a data frame in an R list.\n\n\n## Databases and R\n\nVirtually every database in existence makes use of SQL (**S**tructured **Q**uery **L**anguage). However, it seems that its language is *archaic* and less intuitive than R for instance. \n\nIn R, we can interact with databases through the package [dbplyr](https://dbplyr.tidyverse.org/){target=\"_blank\"} allowing us to use remote database tables as if they are in-memory data frames by automatically converting dplyr code into SQL. Apparently, this package suggest installing the package [DBI](https://dbi.r-dbi.org/){target=\"_blank\"}, which helps connecting R to database management systems (DBMS).^[\"software system that enables users to define, create, maintain and control access to the database.\"]\n\n\nWhile **DBI** is automatically bundled with **dbplyr**, you’ll need to install a specific backend^[The part of a computer system or application that is not directly accessed by the user, typically responsible for storing and manipulating data] package for the type of database that you want to connect to. You can see a list of commonly used backends [here](https://github.com/r-dbi/backends#readme){target=\"_blank\"}. For today, however, we’ll focus on two:\n\n  1. **duckdb** embeds a DuckDB database.\n  2. **bigrquery** connects to Google BigQuery.\n\nThe first one is more \"local\" since it allows you to have the databases in your pc. With other **DBMS** you need an external server.\n\n## Getting started: DuckDB\n\n- The goal is to set up a simple local database using [DuckDB](https://duckdb.org/docs/stable/clients/r.html){target=\"_blank\"}.\n\n- We'll learn how to **connect and run queries from R** easily.\n\n- The principles covered will **apply to larger and more complex datasets** later on.\n\n### Connectic to a database\n\nStart by opening an (empty) database connection via the `DBI::dbConnect()` function, which we’ll call `con`. With `:memory:` we are telling R that this is a local connection that exists in memory\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(DBI) ## Already loaded\n\ncon <- dbConnect(duckdb::duckdb(), path = \":memory:\")\n```\n:::\n\n\n\n- `DBI::dbConnect()` takes different arguments depending on the database, but the first is always the backend (e.g., `duckdb::duckdb()` for DuckDB).\n\n- For DuckDB, the only other required argument is the database `path` — \":memory:\" creates a temporary in-memory database.\n\n- More complex connections (e.g., remote databases) may later require **additional settings like passwords**.\n\nNext, we copy the `flights` dataset into our DuckDB database using `dplyr::copy_to()`. We specify a table name and optionally define **indexes** to improve performance. In real-world scenarios, **index management is usually handled automatically** by the database system.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(dplyr)        ## Already loaded\n# library(nycflights13) ## Already loaded\n\ncopy_to(\n  dest = con, \n  df = nycflights13::flights, \n  name = \"flights\",\n  temporary = FALSE, \n  indexes = list(\n    c(\"year\", \"month\", \"day\"), \n    \"carrier\", \n    \"tailnum\",\n    \"dest\"\n    )\n  )\n```\n:::\n\n\n\nNow that we’ve copied over the data, we can reference it from R via the `dplyr::tbl()` function. This will allow us to treat it as a normal data frame that be manipulated with **dplyr** commands. Remember, we name our table as `flights`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## List tables in our DuckDB database connection (optional)\n# dbListTables(con)\n\n## Reference the table from R\nflights_db <-  tbl(con, \"flights\")\nflights_db\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   table<flights> [?? x 19]\n# Database: DuckDB v1.2.1 [ccardonaa@Windows 10 x64:R 4.4.2/:memory:]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n```\n:::\n:::\n\n\nIt is working, however, the part of `[ccardonaa@Windows 10 x64:R 4.4.2/:memory:]` does not come up when Grant does it in his notes. Probably he has hidden those features before.\n\n### Generating Queries\n\n\n**dplyr** automatically translates tidyverse-style code into SQL behind the scenes. Many **dplyr** verbs mirror SQL commands, so we can use familiar syntax to run queries.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Select some columns\nflights_db |> select(year:day, dep_delay, arr_delay)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [?? x 5]\n# Database: DuckDB v1.2.1 [ccardonaa@Windows 10 x64:R 4.4.2/:memory:]\n    year month   day dep_delay arr_delay\n   <int> <int> <int>     <dbl>     <dbl>\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# ℹ more rows\n```\n:::\n:::\n\n\nNext, we filter according to a condition:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Filter according to some condition\nflights_db |> filter(dep_delay > 240) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [?? x 19]\n# Database: DuckDB v1.2.1 [ccardonaa@Windows 10 x64:R 4.4.2/:memory:]\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n 1  2013     1     1      848           1835       853     1001           1950\n 2  2013     1     1     1815           1325       290     2120           1542\n 3  2013     1     1     1842           1422       260     1958           1535\n 4  2013     1     1     2115           1700       255     2330           1920\n 5  2013     1     1     2205           1720       285       46           2040\n 6  2013     1     1     2343           1724       379      314           1938\n 7  2013     1     2     1332            904       268     1616           1128\n 8  2013     1     2     1412            838       334     1710           1147\n 9  2013     1     2     1607           1030       337     2003           1355\n10  2013     1     2     2131           1512       379     2340           1741\n# ℹ more rows\n# ℹ 11 more variables: arr_delay <dbl>, carrier <chr>, flight <int>,\n#   tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Get the mean delay by destination (group and then summarise)\nflights_db |>\n  group_by(dest) |>\n  summarise(mean_dep_delay = mean(dep_delay))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [?? x 2]\n# Database: DuckDB v1.2.1 [ccardonaa@Windows 10 x64:R 4.4.2/:memory:]\n   dest  mean_dep_delay\n   <chr>          <dbl>\n 1 HOU            14.3 \n 2 PHX            10.4 \n 3 SYR            14.4 \n 4 BDL            17.7 \n 5 BWI            16.4 \n 6 SAT            20.7 \n 7 PSP            -2.94\n 8 ATL            12.5 \n 9 MCO            11.3 \n10 BNA            16.0 \n# ℹ more rows\n```\n:::\n:::\n\n\nEverything seems working smoothly. The question is: what is `Source:   SQL`? This part is different with his notes because there you have `# Source:   lazy query`.\n\n### Laziness as a virtue\n\n**dplyr** works lazily by translating your code into SQL and running it in the database rather than in R. This means it won’t pull data into R or do any actual work until it absolutely has to, making things more efficient.\n\nFor example, consider an example where we are interested in the mean departure and arrival delays for each plane (i.e. by unique tail number). I’ll also drop observations with less than 100 flights.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntailnum_delay_db <-  \n  flights_db |> \n  group_by(tailnum) |>\n  summarise(\n    mean_dep_delay = mean(dep_delay),\n    mean_arr_delay = mean(arr_delay),\n    n = n()\n    ) |>\n  filter(n > 100) |> \n  arrange(desc(mean_arr_delay))\n```\n:::\n\n\n\nEven after chaining operations, the database isn’t accessed right away — dplyr waits until you request the data. When you do, it generates the SQL and retrieves only what’s needed, like just a few rows for printing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntailnum_delay_db\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:     SQL [?? x 4]\n# Database:   DuckDB v1.2.1 [ccardonaa@Windows 10 x64:R 4.4.2/:memory:]\n# Ordered by: desc(mean_arr_delay)\n   tailnum mean_dep_delay mean_arr_delay     n\n   <chr>            <dbl>          <dbl> <dbl>\n 1 N11119            32.6           30.3   148\n 2 N16919            32.4           29.9   251\n 3 N14998            29.4           27.9   230\n 4 N15910            29.3           27.6   280\n 5 N13123            29.6           26.0   121\n 6 N11192            27.5           25.9   154\n 7 N14950            26.2           25.3   219\n 8 N21130            27.0           25.0   126\n 9 N24128            24.8           24.9   129\n10 N22971            26.5           24.7   230\n# ℹ more rows\n```\n:::\n:::\n\n\n### Collect the data into your local R environment\n\nYou’ll often refine your query a few times before deciding what data to pull. Once ready, use `collect()` to bring the results into R — here, the data is saved as a new object to keep the database version separate for later examples.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntailnum_delay <-  \n  tailnum_delay_db |> \n  collect()\ntailnum_delay\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1,201 × 4\n   tailnum mean_dep_delay mean_arr_delay     n\n   <chr>            <dbl>          <dbl> <dbl>\n 1 N11119            32.6           30.3   148\n 2 N16919            32.4           29.9   251\n 3 N14998            29.4           27.9   230\n 4 N15910            29.3           27.6   280\n 5 N13123            29.6           26.0   121\n 6 N11192            27.5           25.9   154\n 7 N14950            26.2           25.3   219\n 8 N21130            27.0           25.0   126\n 9 N24128            24.8           24.9   129\n10 N22971            26.5           24.7   230\n# ℹ 1,191 more rows\n```\n:::\n:::\n\n\n\nSuper. We have successfully pulled the queried database into our local R environment as a data frame. You can now proceed to use it in exactly the same way as you would any other data frame. For example, we could plot the data to see i) whether there is a relationship between mean departure and arrival delays (there is), and ii) whether planes manage to make up some time if they depart late (they do).\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntailnum_delay |>\n  ggplot(aes(x=mean_dep_delay, y=mean_arr_delay, size=n)) +\n  geom_point(alpha=0.3) +\n  geom_abline(intercept = 0, slope = 1, col=\"orange\") +\n  coord_fixed()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.svg){fig-align='center' width=80%}\n:::\n:::\n\n\n### Joins\n\nDatabases are great at handling joins, and **dplyr**’s join functions map directly to their SQL counterparts. This makes translating joins from **tidyverse** syntax to SQL seamless and intuitive. Also, like lists, databases can store multiple tables — here we add `planes` to the same DuckDB connection as `flights.`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Copy over the \"planes\" dataset to the same \"con\" DuckDB connection.\ncopy_to(\n    dest = con, \n    df = nycflights13::planes, \n    name = \"planes\",\n    temporary = FALSE, \n    indexes = \"tailnum\"\n    )\n\n## List tables in our \"con\" database connection (i.e. now \"flights\" and \"planes\")\ndbListTables(con)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"flights\" \"planes\" \n```\n:::\n:::\n\n\nThen, we join the `flights_db` and `planes_db` tables by matching the `tailnum` column, after renaming the `year` column in `planes_db` to `year_built.` It then selects a subset of relevant columns to view flight and aircraft details together in one table.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Reference from dplyr\nplanes_db <-  tbl(con, 'planes')\n\n## Run the equivalent left join that we saw back in the tidyverse lecture\nleft_join(\n    flights_db,\n    planes_db |> rename(year_built = year),\n    by = \"tailnum\" ## Important: Be specific about the joining column\n) |>\n    select(year, month, day, dep_time, arr_time, carrier, flight, tailnum,\n           year_built, type, model) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Source:   SQL [?? x 11]\n# Database: DuckDB v1.2.1 [ccardonaa@Windows 10 x64:R 4.4.2/:memory:]\n    year month   day dep_time arr_time carrier flight tailnum year_built type   \n   <int> <int> <int>    <int>    <int> <chr>    <int> <chr>        <int> <chr>  \n 1  2013     1     1      517      830 UA        1545 N14228        1999 Fixed …\n 2  2013     1     1      544     1004 B6         725 N804JB        2012 Fixed …\n 3  2013     1     1      554      812 DL         461 N668DN        1991 Fixed …\n 4  2013     1     1      554      740 UA        1696 N39463        2012 Fixed …\n 5  2013     1     1      555      913 B6         507 N516JB        2000 Fixed …\n 6  2013     1     1      557      709 EV        5708 N829AS        1998 Fixed …\n 7  2013     1     1      557      838 B6          79 N593JB        2004 Fixed …\n 8  2013     1     1      558      849 B6          49 N793JB        2011 Fixed …\n 9  2013     1     1      558      923 UA        1124 N53441          NA Fixed …\n10  2013     1     1      559      702 B6        1806 N708JB        2008 Fixed …\n# ℹ more rows\n# ℹ 1 more variable: model <chr>\n```\n:::\n:::\n\n\nNormally, we’d disconnect from DuckDB using `DBI::dbDisconnect(con)` once we're done querying.\nBut here, we’ll keep the connection open a bit longer to show how to run raw SQL directly from R.\n\n## Using SQL directly in R\n\n### Translate with dplyr::show_query()\n\nBehind the scenes, **dplyr** is translating your R code into SQL. You can use the show_query() function to display the SQL code that was used to generate a queried table.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntailnum_delay_db |> show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT\n  tailnum,\n  AVG(dep_delay) AS mean_dep_delay,\n  AVG(arr_delay) AS mean_arr_delay,\n  COUNT(*) AS n\nFROM flights\nGROUP BY tailnum\nHAVING (COUNT(*) > 100.0)\nORDER BY mean_arr_delay DESC\n```\n:::\n:::\n\n\n- SQL code is often less intuitive and more verbose than **dplyr** due to translation steps and SQL's structure.\n\n- The **dplyr** engine adds safeguards to ensure valid SQL, which can lead to redundant or repetitive code.\n\n- SQL enforces a **strict lexical order of execution**, meaning every query must follow a fixed sequence of commands, which may not match the logical flow of analysis.\n\nYou might wonder if learning SQL is necessary since **dplyr** works so well — and the answer is yes, eventually you'll need it. Fortunately, writing SQL directly from R is easy with the **DBI** package, and we’ll explore two ways to do it next.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Show the equivalent SQL query for these dplyr commands\nflights_db |> \n  select(month, day, dep_time, sched_dep_time, dep_delay) |>\n  filter(dep_delay > 240) |> \n  head(5) |> \n  show_query()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<SQL>\nSELECT \"month\", \"day\", dep_time, sched_dep_time, dep_delay\nFROM flights\nWHERE (dep_delay > 240.0)\nLIMIT 5\n```\n:::\n:::\n\n\n### Use DBI:dbGetQuery()\n\nTo run SQL queries in regular R scripts, we can use the `DBI::dbGetQuery()` function. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Run the query using SQL directly on the connection.\ndbGetQuery(con, \"SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1 2013     1   1      848           1835       853     1001           1950\n2 2013     1   1     1815           1325       290     2120           1542\n3 2013     1   1     1842           1422       260     1958           1535\n4 2013     1   1     2115           1700       255     2330           1920\n5 2013     1   1     2205           1720       285       46           2040\n  arr_delay carrier flight tailnum origin dest air_time distance hour minute\n1       851      MQ   3944  N942MQ    JFK  BWI       41      184   18     35\n2       338      EV   4417  N17185    EWR  OMA      213     1134   13     25\n3       263      EV   4633  N18120    EWR  BTV       46      266   14     22\n4       250      9E   3347  N924XJ    JFK  CVG      115      589   17      0\n5       246      AA   1999  N5DNAA    EWR  MIA      146     1085   17     20\n            time_hour\n1 2013-01-01 23:00:00\n2 2013-01-01 18:00:00\n3 2013-01-01 19:00:00\n4 2013-01-01 22:00:00\n5 2013-01-01 22:00:00\n```\n:::\n:::\n\n\nLet's try with a few variables just to check:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Run the query using SQL directly on the connection.\ndbGetQuery(con, \"SELECT month, day, dep_time, sched_dep_time, dep_delay FROM flights WHERE dep_delay > 240.0 LIMIT 5\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  month day dep_time sched_dep_time dep_delay\n1     1   1      848           1835       853\n2     1   1     1815           1325       290\n3     1   1     1842           1422       260\n4     1   1     2115           1700       255\n5     1   1     2205           1720       285\n```\n:::\n:::\n\n\n### Recommendation: Use glue::glue_sql()\n\nUsing `dbGetQuery()` with a full SQL string works, but `glue_sql()` from the [glue](https://glue.tidyverse.org/){target=\"_blank\"} package is more flexible. It lets you safely insert R variables into SQL queries and split long queries into manageable parts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(glue) ## Already loaded\n\n## Some local R variables\ntbl <- \"flights\"\nd_var <- \"dep_delay\"\nd_thresh <- 240\n\n## The \"glued\" SQL query string\nsql_query <-\n  glue_sql(\"\n  SELECT *\n  FROM {`tbl`}\n  WHERE ({`d_var`} > {d_thresh})\n  LIMIT 5\n  \",\n  .con = con\n  )\n\n## Run the query\ndbGetQuery(con, sql_query)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n1 2013     1   1      848           1835       853     1001           1950\n2 2013     1   1     1815           1325       290     2120           1542\n3 2013     1   1     1842           1422       260     1958           1535\n4 2013     1   1     2115           1700       255     2330           1920\n5 2013     1   1     2205           1720       285       46           2040\n  arr_delay carrier flight tailnum origin dest air_time distance hour minute\n1       851      MQ   3944  N942MQ    JFK  BWI       41      184   18     35\n2       338      EV   4417  N17185    EWR  OMA      213     1134   13     25\n3       263      EV   4633  N18120    EWR  BTV       46      266   14     22\n4       250      9E   3347  N924XJ    JFK  CVG      115      589   17      0\n5       246      AA   1999  N5DNAA    EWR  MIA      146     1085   17     20\n            time_hour\n1 2013-01-01 23:00:00\n2 2013-01-01 18:00:00\n3 2013-01-01 19:00:00\n4 2013-01-01 22:00:00\n5 2013-01-01 22:00:00\n```\n:::\n:::\n\n\nI know this seems like more work (undeniably so for this simple example). However, the `glue::glue_sql()` approach really pays off when you start working with bigger, nested queries. See the [documentation](https://glue.tidyverse.org/reference/glue_sql.html){target=\"_blank\"} for more examples and functionality, including how to match on or iterate over or multiple input values.\n\n\n### Disconnect\n\nFinally, disconnect from the connection using the DBI::dbDisconnect() function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbDisconnect(con)\n```\n:::\n\n\n## Scaling up: Google BigQuery\n\nGoogle BigQuery is a fast, scalable, and easy-to-use platform that's great for data analysis. Some of its key advantages include:\n\n- **Accessibility**: Available through Google Cloud Platform, which you likely joined during the cloud computing lecture.\n\n- **Economy**: Lets you query up to 1 TB per month for free, with additional usage costing just $5 per TB.\n\n- **Data availability**: Offers sample tables and a wide range of public datasets to explore.\n\n\n::: {.cell}\n\n:::\n\n\nFrom now on until the end of the section, I am going to describe my own experience with the package since I do not have all the Google Cloud settings apart from an account. The documentation of the [bigrquery](https://bigrquery.r-dbi.org/){target=\"_blank\"} package actually was very useful!\n\nTo start with, the only project that I have in the Google Cloud is \"*My First Project*\", so I simply copy and paste its ID and I created:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The project_id is inside the quotes\nbilling <- \"project_id\"\n```\n:::\n\n\nThen, I run what is the lecture notes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbq_con <- \n  dbConnect(\n    bigrquery::bigquery(),\n    project = \"publicdata\",\n    dataset = \"samples\",\n    billing = project_id\n    )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndbListTables(bq_con)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"github_nested\"   \"github_timeline\" \"gsod\"            \"natality\"       \n[5] \"shakespeare\"     \"trigrams\"        \"wikipedia\"      \n```\n:::\n:::\n\n\n\n### US birth data\n\nFor this section we use the `natality` table:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnatality <-  tbl(bq_con, \"natality\")\n```\n:::\n\n\n\nAs a reference point, the raw natality data on BigQuery is about 22 GB. Not gigantic, but enough to overwhelm most people’s RAM. Here’s a simple exercise where we collapse the data down to yearly means.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbw <- natality |>\n  filter(!is.na(state)) |> ## optional to remove some outliers\n  group_by(year) |>\n  summarise(weight_pounds = mean(weight_pounds, na.rm=TRUE)) |>\n  collect()\n```\n:::\n\n\nLet's plot it:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbw |>\n  ggplot(aes(year, weight_pounds)) +\n  geom_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.svg){fig-align='center' width=80%}\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}